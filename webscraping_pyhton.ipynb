{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscraping com Python\n",
    "\n",
    "Tutorial de webscraping com pyhton utilizando a biblioteca BeautifulSoup\n",
    "\n",
    "### O que é webscraping?\n",
    "\n",
    "Webscraping é uma técnica de extração de dados, com ela podemos coletar dados de sites. Fazemos a 'raspagem' dos dados  que são interessantes para nós.\n",
    "\n",
    "Por exemplo: resgatar os últimos posts que foram escritos em vários blogs, assim sem precisarmos entrar nos sites alvo, podemos simplesmente iterar sobre eles e resgatar os dados, este seria um exemplo simples da aplicação da técnica.\n",
    "\n",
    "O que é muito mais rápido do que entrar em blog por blog e verificar o conteúdo e copiar para alguma planinha.\n",
    "\n",
    "Há muitas empresas que utilizam como forma de gerar recursos, um exemplo clássico é o site Buscapé, ele varre os sites que vendem os produtos pesquisados em busca dos menores preços.\n",
    "\n",
    "### Mais sobre o webscraping:\n",
    "\n",
    "Depois da extração dos dados, podemos armazenar eles de diversas formas:\n",
    "\n",
    "- Salvar em um banco de dados;\n",
    "- Salvar em CSV;\n",
    "- Salvar em XLS;\n",
    "- Entre outros;\n",
    "\n",
    "O processo em si é bem básico, definimos os dados que queremos, escolhemos os sites, montamos o script e recebemos os dados para análise, este é o ciclo de vida do webscraping.\n",
    "\n",
    "Outro ponto legal é que se você souber um pouco de programação web vai se sentir muito confortável na exploração das tags, e na própria estrutura do HTML, isso já te deixa na frente.\n",
    "\n",
    "### Porque utilizar?\n",
    "\n",
    "Bom, vamos imaginar um caso: precisamos dos comentários dos usuários de um produto X em 200 e-commerces diferentes para analisa-los e validar se o produto tem aceitação no mercado.\n",
    "\n",
    "Poderiamos entrar nos 200 sites copiar todos os comentários e salvar de alguma forma, por exemplo, mas e quantas horas gastariamos nisso? E se a pesquisa precisa ser repetida toda semana? Se faltou algum dado na primeira pesquisa?\n",
    "\n",
    "Teriamos que novamente nos submetermos a todo este processo manual, gastando horas para gerar algo super simples!\n",
    "\n",
    "É aí que entra o webscraping, automatizamos todo o processo e só precisamos rodar o script até quando precisarmos, é muito mais vantajoso, e se feito de maneira correta se torna muito mais preciso também, a prova de erros humanos.\n",
    "\n",
    "### Antes de iniciarmos a prática\n",
    "\n",
    "Vou utilizar o Jupyter Notebook para demonstrar todos os exemplos, instalando o Anaconda você já consegue todas as libs usadas neste tutorial e inclusive o Notebook, que é uma forma excelente para executarmos nossos testes.\n",
    "\n",
    "Caso você opte por instalar as libs separadamente, faça isso com o pip, o resultado final será o mesmo, gosto da solução do Anaconda pois é prática e rápida, além de ser muito utilizada para tutoriais e compartilhamento de notebooks.\n",
    "\n",
    "Fiz um artigo de como instalar o Jupyter em diversas plataformas, caso queira aproveitar: [confira aqui!](https://medium.com/matheusbudkewicz/como-instalar-o-jupyter-notebook-windows-e-linux-20701fc583c)\n",
    "\n",
    "### Projeto no final\n",
    "\n",
    "Outro aviso é para os que gostam de prática, no fim do artigo teremos um mini projeto que simula uma situação do dia a dia, explorando todos os conceitos aprendidos durante a leitura!\n",
    "\n",
    "### Repositório\n",
    "\n",
    "Todo o conteúdo se encontra no Github caso queira analisar o código: [ir para o repositório!](https://github.com/matheusbattisti/webscraping_python)\n",
    "\n",
    "### Hello World em webscraping\n",
    "\n",
    "Quem está nesse mundo da programação sabe que precisamos fazer um Hello World o quanto antes, é parte da tradição!\n",
    "\n",
    "Vamos colocar a mão na massa para adicionar um pouco de prática, assim já conseguiremos abordar tópicos mais avançados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<html>\\n<head>\\n<title>A Useful Page</title>\\n</head>\\n<body>\\n<h1>An Interesting Title</h1>\\n<div>\\nLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\\n</div>\\n</body>\\n</html>\\n'"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importando as libs\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# Definimos a url alvo. obs: esta url vem de um livro sobre o assunto do tópico\n",
    "url = 'http://pythonscraping.com/pages/page1.html'\n",
    "\n",
    "# Aplicamos uma requisição para pegar o HTML\n",
    "html = urlopen(url)\n",
    "\n",
    "# Verificação do conteúdo\n",
    "html.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pronto!\n",
    "\n",
    "Temos o primeiro resultado por webscraping, foi fácil não?\n",
    "\n",
    "O retorno do método urlopen foi um HTML simples com diversos elementos que vemos todos os dias, o próximo passo seria extrair os dados que nós desejamos.\n",
    "\n",
    "Outro ponto importante é que no passo acima utilizamos a lib urllopen, nativa do Python, porém em outros tutoriais você pode se deparar com a Requests, que é bem conhecida e a comunidade abraça bem ela, então porque utilizamos a urllib?\n",
    "\n",
    "\n",
    "### Requests ou urllib?\n",
    "\n",
    "Bom, como vamos fazer requisições HTTP, no caso get, nas páginas que queremos extrair dados, precisamos de uma lib para isso, durante meus estudos sobre o tema estas duas apareceram de forma igual, então fui atrás para saber as vantagens e desvantagens de cada uma.\n",
    "\n",
    "Basicamente o que encontrei é que Requests é uma lib externa, no caso estariamos criando uma dependência para o projeto, já um ponto forte é a simplicidade de escrever o código, escrevemos menos linhas para chegar ao mesmo resultado comparando com a urllib, além disso há alguns comentários que dizem que Requests tem um código mais limpo e moderno que a outra.\n",
    "\n",
    "Já urllib é nativa do Python, o que quer dizer que provavelmente será mantida pela mesma equipe que trabalha na linguagem e até enquanto Python durar ou fizeram outra versão que resolva este problema, há alguns comentários pelo stackoverflow que falam sobre o desenvolvimento dela ser feito numa lógica de 'resolver o problema' ao invés de código limpo e performático.\n",
    "\n",
    "Neste post pretendo seguir com a urllib, mas teremos um exemplo com a Requests para você poder escolher a que mais te agrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<html>\\n<head>\\n<title>A Useful Page</title>\\n</head>\\n<body>\\n<h1>An Interesting Title</h1>\\n<div>\\nLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\\n</div>\\n</body>\\n</html>\\n'"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importando a Requests\n",
    "import requests\n",
    "\n",
    "# Vamos testar a biblioteca requests\n",
    "html = requests.get(url)\n",
    "\n",
    "# Diferente da urllib, usamos text para apresentar o conteudo que o get nos trouxe\n",
    "html.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que vimos como conseguir os dados de um site, está na hora de avançarmos ao próximo nível: usar uma biblioteca para poder manipular o HTML de uma forma mais fácil\n",
    "\n",
    "### BeautifulSoup\n",
    "\n",
    "Com a BeautifulSoup tudo será mais fácil, esta biblioteca do Python serve para extrairmos dados de HTML e XML, de forma fácil e descomplicada podemos acessar os 'nós' da estrutura do HTML da página ou até mesmo classes e pegar as informações\n",
    "\n",
    "Vamos utilizar ela num exemplo para que fique claro o quanto mais fácil será se locomover pelas tags HTML ao invés de texto puro, como nos retornos dos métodos anteriores\n",
    "\n",
    "No próximo exemplo vou pegar o título da página"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Welcome to Python.org</title>\n"
     ]
    }
   ],
   "source": [
    "# Importando a BeautifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Vamos mudar a URL\n",
    "url = 'https://www.python.org/'\n",
    "\n",
    "# lendo a URL com a urllopen\n",
    "html = urlopen(url)\n",
    "\n",
    "# Enfim mostrando o poder da bs4\n",
    "bs = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "# Imprimindo o título da página\n",
    "print(bs.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Já neste exemplo eu procuro por todas as tags h1 da página com o método find_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<h1 class=\"site-headline\">\n",
      "<a href=\"/\"><img alt=\"python™\" class=\"python-logo\" src=\"/static/img/python-logo.png\"/></a>\n",
      "</h1>, <h1>Functions Defined</h1>, <h1>Compound Data Types</h1>, <h1>Intuitive Interpretation</h1>, <h1>Quick &amp; Easy to Learn</h1>, <h1>All the Flow You’d Expect</h1>]\n"
     ]
    }
   ],
   "source": [
    "# Imprimindo todas as h1s da página\n",
    "print(bs.find_all('h1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introdução de HTML\n",
    "\n",
    "Vou apresentar uma rápida introdução ao HTML, caso você já se sinta confortável para entender a estrutura ou já trabalhou com desenvolvimento web, pule para a próxima parte. :)\n",
    "\n",
    "A seguir uma estrutura HTML simples:\n",
    "\n",
    "\n",
    "![HTML](img/ws4.png)\n",
    "\n",
    "Toda página HTML tem pelo menos 3 tags que vão definir o esqueleto principal:\n",
    "\n",
    "- **< html >:** ela define que este é um documento HTML;\n",
    "- **< head >:** dentro destas tags serão colocadas configurações da página como encode, adicionados outros arquivos externos como scripts, definido o título da página ( o que aparece na aba do browser ), entre outras que são como parâmetros para a página web;\n",
    "- **< body >:** esta tag contém todos os elementos visíveis da página, textos, listas, parágrafos e etc;\n",
    "\n",
    "Tudo que está entre sinais de < e > são tags, é por meio delas que estruturamos a página, a grande maioria precisa ser aberta e fechada, como no exemplo a seguir:\n",
    "\n",
    "< p >Texto do parágrafo< /p >\n",
    "\n",
    "Porém há algumas que não tem essa necessidade, mas não vem ao caso neste post, fogem do escopo.\n",
    "\n",
    "Como observado antes no nosso primeiro exemplo de webscraping, requisitamos title que é a tag title \n",
    "\n",
    "HTML é uma linguagem de marcação, não de programação, ela nos ajuda a estruturar as páginas, com elementos que podem representar blocos que dividem as páginas em partes (div), elementos de texto como título (h1), listas (ul), tabelas (table) e por aí vai, sempre seguindo esta lógica de cada tag conter um conteúdo diferente, lembrando que tudo que é visível fica dentro da tag body.\n",
    "\n",
    "As tags tambem aceitam parâmetros, os mais utilizados são classes e ids:\n",
    "\n",
    "- **classe:** o intuito de adicionar uma classe é que vários elementos possam aproveitar suas características\n",
    "- **id:** quando colocamos um id num elemento, ele tende a ser único na página, nas boas práticas dois elementos diferentes não podem ter o mesmo id\n",
    "\n",
    "Quando falo características me refiro a CSS ou manipulação do DOM com JS, não vou entrar nesse assunto também, mas é basicamente estilização dos elementos ou manipulação deles por meio destas duas linguagens.\n",
    "\n",
    "Voltando para o webscraping, as classes e ids nos auxiliarão a acessar os elementos que queremos resgatar os dados.\n",
    "\n",
    "Por exemplo: identificamos que um dado importante está em um parágrafo, se procurarmos por parágrafos podemos achar inúmeros elementos com essa tag, agora se o elemento alvo conter um id ele será apenas um, e isso facilitará muito nossa vida.\n",
    "\n",
    "Creio que com isso, o básico do HTML está exemplificado e além disso conseguimos estabelecer ligações entre o webscraping e a estrutura HTML, como iremos navegar para resgatar os dados!\n",
    "\n",
    "Vamos retornar para a BeautifulSoup\n",
    "\n",
    "### Métodos find() e find_all()\n",
    "\n",
    "O método find_all procura por todo o documento por resultados da nossa pesquisa, e nos retornas toda as ocorrências encontradas.\n",
    "\n",
    "Porém as vezes queremos encontrar apenas um elemento, o que nos dá duas possibilidades\n",
    "\n",
    "- passar o argumento limit = 1, no find_all()\n",
    "- utilizar o find()\n",
    "\n",
    "Com o find() o primeiro elemento encontrado é retornado, assim caso buscassemos uma tabela e soubermos que apenas uma tabela existe em todo o HTML é desnecessário utilizar o find_all() que leria o documento todo, assim por questões até de performance optariamos pelo find()\n",
    "\n",
    "Veremos agora os dois na prática:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<h1 class=\"header\">Top Rated Movies</h1>]\n",
      "\n",
      "\n",
      "Foram encontradas 1 ocorrências de h1\n"
     ]
    }
   ],
   "source": [
    "# Vamos permanecer na url que o site do Python se contra e vamos verificar todas as tags h1\n",
    "print(bs.find_all('h1'))\n",
    "\n",
    "# pulando linha\n",
    "print('\\n')\n",
    "\n",
    "# Número de h1's encontrados\n",
    "print('Foram encontradas {} ocorrências de h1'.format(len(bs.find_all('h1'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O que aconteceu?\n",
    "\n",
    "Usamos bs que é a instância de BeautifulSoup criada anteriormente para extrair dados da página.\n",
    "\n",
    "Conseguimos pegar todos os h1's presentes na home do site, vimos que 6 elementos foram encontrados.\n",
    "\n",
    "E caso quisessemos apenas o primeiro?\n",
    "\n",
    "find() ao resgate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h1 class=\"site-headline\">\n",
       "<a href=\"/\"><img alt=\"python™\" class=\"python-logo\" src=\"/static/img/python-logo.png\"/></a>\n",
       "</h1>"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Agora com find, para apenas o primeiro resultado ser extraído\n",
    "bs.find('h1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O primeiro h1 foi encontrado e o método ja nos retornou\n",
    "\n",
    "Como explicado anteriormente, em vez de ler todo o HTML a bs4 apenas achou o elemento alvo e nos retornou.\n",
    "\n",
    "Simples não?\n",
    "\n",
    "### Selecionando por class e id\n",
    "\n",
    "Agora vamos mudar de URL, para termos mais exemplos e trabalharmos com estruturas diferentes.\n",
    "\n",
    "O que será abordado neste tópico é a seleção de elementos por class e id, como vimos anteriormente no HTML estes dois atributos são amplamente utilizados e vão nos guiar até os elementos corretos\n",
    "\n",
    "BeautifulSoup não poderia ficar para trás, criou formas de selecionarmos os elementos pelos dois.\n",
    "\n",
    "### Inspecionando elemento\n",
    "\n",
    "Vale a pena ressaltar aqui também, caso você não tenha muita experiência com web, a funcionalidade de inspecionar elementos do HTML\n",
    "\n",
    "Com ela é que vamos descobrir as classes e ids que queremos.\n",
    "\n",
    "- **No Chrome:** podemos acessar clicando com o botão direito na página e selecionando 'Inspecionar'\n",
    "- **No Firefox:** podemos acessar clicando com o botão direito na página e selecionando 'Inspecionar Elemento'\n",
    "\n",
    "Nos demais brownsers a lógica é a mesma, não tem muito mistério.\n",
    "\n",
    "\n",
    "### Voltando ao exemplo\n",
    "\n",
    "Vamos inspecionar o título principal do site da [Wikipedia](https://www.wikipedia.org/)\n",
    "\n",
    "![Wikipedia](img/ws1.png)\n",
    "\n",
    "Agora vamos ver a estrutura HTML\n",
    "\n",
    "![DOM](img/ws2.png)\n",
    "\n",
    "Temos uma div com uma classe chamada 'central-textlogo__image', é exatamente ela que vamos selecionar para extrair o texto do título, veja abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"central-textlogo__image sprite svg-Wikipedia_wordmark\">\n",
      "Wikipedia\n",
      "</div>\n",
      "\n",
      "Wikipedia\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Trocando URL\n",
    "url = 'https://www.wikipedia.org/'\n",
    "\n",
    "# lendo a URL com a urllopen\n",
    "html = urlopen(url)\n",
    "\n",
    "# Enfim mostrando o poder da bs4\n",
    "bs = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "# Imprimindo o título da página\n",
    "print(bs.find('div', class_='central-textlogo__image'))\n",
    "\n",
    "# Se quisermos apenas o texto, podemos utilizar a propriedade text\n",
    "print(bs.find('div', class_='central-textlogo__image').text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conseguimos ter o retorno esperado, pela classe conseguimos filtrar melhor o que queremos extrair\n",
    "\n",
    "### Selecionando por id\n",
    "\n",
    "Agora vamos ver com o parâmetro id, como devemos utilizar para selecionar o elemento alvo\n",
    "\n",
    "Mais abaixo no HTML temos as seleções de linguagem, vamos selecionar a que se refere a língua francesa\n",
    "\n",
    "![linguagem francês](img/ws3.png)\n",
    "\n",
    "O elemento com id 'js-link-box-fr' é o que precisamos selecionar\n",
    "\n",
    "![id](img/ws5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a class=\"link-box\" data-slogan=\"L’encyclopédie libre\" href=\"//fr.wikipedia.org/\" id=\"js-link-box-fr\" title=\"Français — Wikipédia — L’encyclopédie libre\">\n",
      "<strong>Français</strong>\n",
      "<small><bdi dir=\"ltr\">2 031 000+</bdi> <span>articles</span></small>\n",
      "</a>\n",
      "\n",
      "Français\n",
      "2 031 000+ articles\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecionando a língua francesa\n",
    "print(bs.find(id='js-link-box-fr'))\n",
    "\n",
    "# Imprimindo apenas o texto\n",
    "print(bs.find(id='js-link-box-fr').text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mais preciso\n",
    "\n",
    "Agora temos maneiras de selecionar dados de formas mais precisas, sem ter que navegador muito pelo HTML, com classes e ids nosso trabalho é facilitado\n",
    "\n",
    "\n",
    "### Seletores CSS\n",
    "\n",
    "Outra maneira de selecionar dados são os seletores CSS, como vimos anteriormente no HTML as tags são aninhadas uma nas outras, assim sendo podemos nos guiar por esta estrutura para selecionar algum elemento, exemplos:\n",
    "\n",
    "- **p a:** todas as tags a dentro de um parágrafo\n",
    "- **div p:** todos os parágrafos dentro de divs\n",
    "- **div p span:** todos os spans que estão dentro de um parágrafo que estes estão dentro de uma div\n",
    "- **table td**: todos os tds que estão dentro de tables\n",
    "\n",
    "Basicamente selecionamos de acordo com o aninhamento da estrutura, talvez se você conheça um pouco de web já se sinta mais confortável, mas caso não, é bem simples: olhe a árvore do HTML e verifique dentro de que elemento está o seu alvo, quanto mais preciso for, no caso de mais tags forem especificadas, mais refinado seu resultado virá\n",
    "\n",
    "E uma pequena mudança: agora vamos utilizar o método select(), passando como argumento o seletor CSS desejado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<strong class=\"jsl10n localized-slogan\" data-jsl10n=\"slogan\">The Free Encyclopedia</strong>, <strong>English</strong>, <strong>日本語</strong>, <strong>Español</strong>, <strong>Deutsch</strong>, <strong>Русский</strong>, <strong>Français</strong>, <strong>Italiano</strong>, <strong>中文</strong>, <strong>Português</strong>, <strong>Polski</strong>, <strong class=\"jsl10n\" data-jsl10n=\"app-links.title\">Wikipedia apps are now available:</strong>]\n",
      "\n",
      "\n",
      "[<input name=\"family\" type=\"hidden\" value=\"wikipedia\"/>, <input id=\"hiddenLanguageInput\" name=\"language\" type=\"hidden\" value=\"en\"/>, <input accesskey=\"F\" autocomplete=\"off\" autofocus=\"autofocus\" dir=\"auto\" id=\"searchInput\" list=\"suggestions\" name=\"search\" size=\"20\" type=\"search\"/>, <input name=\"go\" type=\"hidden\" value=\"Go\"/>]\n"
     ]
    }
   ],
   "source": [
    "# Vamos selecionar todas as tags strong dentro de divs\n",
    "print(bs.select('div strong'))\n",
    "\n",
    "# pulando linha\n",
    "print('\\n')\n",
    "\n",
    "# Agora todas as tags input dentro de forms que estão dentro de divs\n",
    "print(bs.select('div form input'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos também utilizar os seletores CSS para classes e ids, que são representados por:\n",
    "\n",
    "- **.** : quando colocamos um . (ponto) na frente de algum texto, bs4 assim como CSS vai automaticamente procurar por classes deste nome\n",
    "- **#** : e o mesmo caso acontece para ids, colocando # na frente de algum nome de id\n",
    "\n",
    "Veja nos exemplos abaixo a aplicação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<input name=\"family\" type=\"hidden\" value=\"wikipedia\"/>, <input id=\"hiddenLanguageInput\" name=\"language\" type=\"hidden\" value=\"en\"/>, <input accesskey=\"F\" autocomplete=\"off\" autofocus=\"autofocus\" dir=\"auto\" id=\"searchInput\" list=\"suggestions\" name=\"search\" size=\"20\" type=\"search\"/>, <input name=\"go\" type=\"hidden\" value=\"Go\"/>]\n",
      "\n",
      "\n",
      "[<form action=\"//www.wikipedia.org/search-redirect.php\" class=\"pure-form\" data-el-section=\"search\" id=\"search-form\">\n",
      "<fieldset>\n",
      "<!-- search-redirect.php is project-independent, requires a family -->\n",
      "<input name=\"family\" type=\"hidden\" value=\"wikipedia\"/>\n",
      "<input id=\"hiddenLanguageInput\" name=\"language\" type=\"hidden\" value=\"en\"/>\n",
      "<div class=\"search-input\" id=\"search-input\">\n",
      "<input accesskey=\"F\" autocomplete=\"off\" autofocus=\"autofocus\" dir=\"auto\" id=\"searchInput\" list=\"suggestions\" name=\"search\" size=\"20\" type=\"search\"/>\n",
      "<div class=\"styled-select no-js\">\n",
      "<div class=\"hide-arrow\">\n",
      "<select id=\"searchLanguage\" name=\"language\">\n",
      "<!-- 100,000+ content pages, sorted by romanization -->\n",
      "<option lang=\"ar\" value=\"ar\">العربية</option><!-- Al-ʿArabīyah -->\n",
      "<option lang=\"az\" value=\"az\">Azərbaycanca</option>\n",
      "<option lang=\"bg\" value=\"bg\">Български</option><!-- Bǎlgarski -->\n",
      "<option lang=\"nan\" value=\"nan\">Bân-lâm-gú / Hō-ló-oē</option>\n",
      "<option lang=\"be\" value=\"be\">Беларуская (Акадэмічная)</option><!-- Belaruskaya (Akademichnaya) -->\n",
      "<option lang=\"ca\" value=\"ca\">Català</option>\n",
      "<option lang=\"cs\" value=\"cs\">Čeština</option>\n",
      "<option lang=\"da\" value=\"da\">Dansk</option>\n",
      "<option lang=\"de\" value=\"de\">Deutsch</option>\n",
      "<option lang=\"et\" value=\"et\">Eesti</option>\n",
      "<option lang=\"el\" value=\"el\">Ελληνικά</option><!-- Ellīniká -->\n",
      "<option lang=\"en\" selected=\"selected\" value=\"en\">English</option><!-- English -->\n",
      "<option lang=\"es\" value=\"es\">Español</option>\n",
      "<option lang=\"eo\" value=\"eo\">Esperanto</option>\n",
      "<option lang=\"eu\" value=\"eu\">Euskara</option>\n",
      "<option lang=\"fa\" value=\"fa\">فارسی</option><!-- Fārsi -->\n",
      "<option lang=\"fr\" value=\"fr\">Français</option>\n",
      "<option lang=\"gl\" value=\"gl\">Galego</option>\n",
      "<option lang=\"ko\" value=\"ko\">한국어</option><!-- Hangugeo -->\n",
      "<option lang=\"hy\" value=\"hy\">Հայերեն</option><!-- Hayeren -->\n",
      "<option lang=\"hi\" value=\"hi\">हिन्दी</option><!-- Hindī -->\n",
      "<option lang=\"hr\" value=\"hr\">Hrvatski</option>\n",
      "<option lang=\"id\" value=\"id\">Bahasa Indonesia</option>\n",
      "<option lang=\"it\" value=\"it\">Italiano</option>\n",
      "<option lang=\"he\" value=\"he\">עברית</option><!-- ʿIvrit -->\n",
      "<option lang=\"ka\" value=\"ka\">ქართული</option><!-- Kartuli -->\n",
      "<option lang=\"la\" value=\"la\">Latina</option>\n",
      "<option lang=\"lt\" value=\"lt\">Lietuvių</option>\n",
      "<option lang=\"hu\" value=\"hu\">Magyar</option>\n",
      "<option lang=\"ms\" value=\"ms\">Bahasa Melayu</option>\n",
      "<option lang=\"min\" value=\"min\">Bahaso Minangkabau</option>\n",
      "<option lang=\"nl\" value=\"nl\">Nederlands</option>\n",
      "<option lang=\"ja\" value=\"ja\">日本語</option><!-- Nihongo -->\n",
      "<option lang=\"nb\" value=\"no\">Norsk (Bokmål)</option>\n",
      "<option lang=\"nn\" value=\"nn\">Norsk (Nynorsk)</option>\n",
      "<option lang=\"ce\" value=\"ce\">Нохчийн</option><!-- Noxçiyn -->\n",
      "<option lang=\"uz\" value=\"uz\">Oʻzbekcha / Ўзбекча</option>\n",
      "<option lang=\"pl\" value=\"pl\">Polski</option>\n",
      "<option lang=\"pt\" value=\"pt\">Português</option>\n",
      "<option lang=\"kk\" value=\"kk\">Қазақша / Qazaqşa / قازاقشا</option>\n",
      "<option lang=\"ro\" value=\"ro\">Română</option>\n",
      "<option lang=\"ru\" value=\"ru\">Русский</option><!-- Russkiy -->\n",
      "<option lang=\"cy\" value=\"cy\">Cymraeg</option><!-- Saesneg -->\n",
      "<option lang=\"en\" value=\"simple\">Simple English</option>\n",
      "<option lang=\"ceb\" value=\"ceb\">Sinugboanong Binisaya</option>\n",
      "<option lang=\"sk\" value=\"sk\">Slovenčina</option>\n",
      "<option lang=\"sl\" value=\"sl\">Slovenščina</option>\n",
      "<option lang=\"sr\" value=\"sr\">Српски / Srpski</option>\n",
      "<option lang=\"sh\" value=\"sh\">Srpskohrvatski / Српскохрватски</option>\n",
      "<option lang=\"fi\" value=\"fi\">Suomi</option>\n",
      "<option lang=\"sv\" value=\"sv\">Svenska</option>\n",
      "<option lang=\"ta\" value=\"ta\">தமிழ்</option><!-- Tamiḻ -->\n",
      "<option lang=\"th\" value=\"th\">ภาษาไทย</option><!-- Phasa Thai -->\n",
      "<option lang=\"tr\" value=\"tr\">Türkçe</option><!-- Turkce -->\n",
      "<option lang=\"azb\" value=\"azb\">تۆرکجه</option><!-- Türkce -->\n",
      "<option lang=\"uk\" value=\"uk\">Українська</option><!-- Ukrayins’ka -->\n",
      "<option lang=\"ur\" value=\"ur\">اردو</option><!-- Urdu -->\n",
      "<option lang=\"vi\" value=\"vi\">Tiếng Việt</option>\n",
      "<option lang=\"vo\" value=\"vo\">Volapük</option>\n",
      "<option lang=\"war\" value=\"war\">Winaray</option>\n",
      "<option lang=\"zh\" value=\"zh\">中文</option><!-- Zhōngwén -->\n",
      "</select>\n",
      "<div class=\"styled-select-active-helper\"></div>\n",
      "</div>\n",
      "<i class=\"sprite svg-arrow-down\"></i>\n",
      "</div>\n",
      "</div>\n",
      "<button class=\"pure-button pure-button-primary-progressive\" type=\"submit\">\n",
      "<i class=\"sprite svg-search-icon\"></i>\n",
      "</button>\n",
      "<input name=\"go\" type=\"hidden\" value=\"Go\"/>\n",
      "</fieldset>\n",
      "</form>]\n"
     ]
    }
   ],
   "source": [
    "# Aqui selecionamos pela classe pure-form todos os inputs dentro dela\n",
    "print(bs.select('.pure-form input'))\n",
    "\n",
    "# pulando linha\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# Aqui selecionamos pela classe pure-form todos os inputs dentro dela\n",
    "print(bs.select('#search-form'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validação de erros\n",
    "\n",
    "Agora que já vimos as maneiras mais comuns de selecionar dados, e posso dizer que para operações simples de webscraping isso já está de bom tamanho, precisamos falar de erros\n",
    "\n",
    "Erros podem acontecer por vários motivos:\n",
    "\n",
    "- Servidor\n",
    "- Programação\n",
    "- Mudança do site\n",
    "\n",
    "Quando saímos da prática e o webscraping vira uma atividade do nosso trabalho, precisamos adicionar confiabilidade no código, e com isso tratar erros para que o código não simplesmente exploda quando alguem tentar rodar, mas sim, informe a pessoa sobre o provavelmente aconteceu\n",
    "\n",
    "Vamos ver agora a nível de requisição como é possível fazer uma validação simples, porém efetiva\n",
    "\n",
    "Primeiramente devemos importar o submódulos que tratam estes erros, que são:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando os submódulos que vão ajudar a tratar os erros\n",
    "from urllib.error import HTTPError\n",
    "from urllib.error import URLError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E então podemos fazer uma validação por classificação de erro, que fica assim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server could not be found!\n"
     ]
    }
   ],
   "source": [
    "# Aqui iniciamos a tratar os erros em nível de URL\n",
    "try:\n",
    "    html = urlopen('http://www.python5.com.br')\n",
    "except HTTPError as e:\n",
    "# Erros HTTP\n",
    "    print(e)\n",
    "except URLError as e:\n",
    "    # URL errada\n",
    "    print('The server could not be found!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como exemplo forcei um erro de URL errada, porém caso o servidor retorne erros como 404 ou 503, caíriam no primeiro caso\n",
    "\n",
    "Isso nos ajudará, não só nós mas também quem vai pegar o código depois, a ter um log que fará com que os problemas se identifiquem mais rapidamente\n",
    "\n",
    "É de responsabilidade do desenvolvedor fazer um código consistente e que trate erros, então fora destes exemplos aqui, sempre opte por tratar os erros e enviar mensagens ao usuários do sistema, caso não possam aparecer na tela emita logs\n",
    "\n",
    "\n",
    "### Projeto\n",
    "\n",
    "Agora para fixar os conceitos vamos nos aproximar de algo mais real, que será explorar os dados da lista dos 250 melhores filmes no IMDB, que é um site de review, críticas e notícias de filmes, séries e afins, nossos objetivos serão:\n",
    "\n",
    "- Importar as libs necessárias;\n",
    "- Validar a URL dos 250 filmes;\n",
    "- Resgatar os dados título, direção e escritores, data do filme e nota;\n",
    "- Transferir os dados para um dataset\n",
    "- Salvar num arquivo CSV\n",
    "\n",
    "Nós vamos iniciar do absoluto zero e comentar cada passo dado até o fim do procedimento, incialmente vamos importas as libs que serão utilizadas:\n",
    "\n",
    "#### Importando libs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as libs\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from urllib.error import URLError\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com a pandas vamos trabalhar no dataset, urllib nos auxiliará a fazer as requisições ao site, e a BeautifulSoup que fará a extração de forma fácil\n",
    "\n",
    "Com as bibliotecas importadas, o próximo passo é definir a URL e o termo de pesquisa para que as consultas não sejam fixas, vamos deixar que o usuário do script escolha o termo a pesquisar\n",
    "\n",
    "#### Validando URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL base\n",
    "url = 'https://www.imdb.com/chart/top?ref_=nv_mv_250'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos nossa URL definida, agora vamos fazer a validação da url com os submódulos de urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratando erros da url\n",
    "try:\n",
    "    html = urlopen(url)\n",
    "except HTTPError as e:\n",
    "# Erros HTTP\n",
    "    print(e)\n",
    "except URLError as e:\n",
    "    # URL errada\n",
    "    print('The server could not be found!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A URL é válida, e o servidor não apresentou erros, agora vamos para a etapa de selecionar os dados.\n",
    "\n",
    "Primeiramente devemos analisar o HTML\n",
    "\n",
    "**Para selecionar cada filme:**\n",
    "\n",
    "![select_filme](img/ws6.png)\n",
    "\n",
    "Vamos selecionar todas as tr's dentro dessa tbody com classe lister-list\n",
    "\n",
    "Precisamos também instaciar o objeto da BeautifulSoup com a url escolhida\n",
    "\n",
    "#### Resgatando os dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enfim mostrando o poder da bs4\n",
    "bs = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "# Imprimindo o título da página\n",
    "movies = bs.select('.lister-list tr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiro vamos analisar o HTML para saber quais elementos devemos resgatar\n",
    "\n",
    "**Para o título:**\n",
    "\n",
    "![select_title](img/ws7.png)\n",
    "\n",
    "Para o título vamos selecionar o a que esta dentro de um td com classe titleColumn e pegar o texto dela\n",
    "\n",
    "**Para os diretores e escritores:**\n",
    "\n",
    "![select_authors](img/ws8.png)\n",
    "\n",
    "Vamos aproveitar a estrutura dos títulos e selecionar o parâmetro title, que nos vai dar os diretores e escritores\n",
    "\n",
    "**Para o ano do filme:**\n",
    "\n",
    "![select_year](img/ws9.png)\n",
    "\n",
    "Ainda no td com classe titleColumn, vamos selecionar o span que é o ano do filme\n",
    "\n",
    "**Para as ratings:**\n",
    "\n",
    "![select_year](img/ws10.png)\n",
    "\n",
    "Vamos selecionar o td com a classe imdbRating e pegar o texto da tag strong dentro deste td\n",
    "\n",
    "Agora que sabemos como selecionar todos os dados, vamos ao código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "directors_writers = []\n",
    "years = []\n",
    "ratings = []\n",
    "\n",
    "for movie in movies:\n",
    "    titles.append(movie.find('td', class_='titleColumn').find('a').get_text())\n",
    "    directors_writers.append(movie.find('td', class_='titleColumn').find('a')['title'])\n",
    "    years.append(movie.find('td', class_='titleColumn').find('span').get_text()[1:5])\n",
    "    ratings.append(movie.find('td', class_='imdbRating').find('strong').get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos criar o dataframe com os dados que nós extraímos\n",
    "\n",
    "#### Criando o dataset:\n",
    "\n",
    "Vamos definir o nome das colunas, e testar o dataset com o método head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>director_and_writers</th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Frank Darabont (dir.), Tim Robbins, Morgan Fre...</td>\n",
       "      <td>9.2</td>\n",
       "      <td>Um Sonho de Liberdade</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Francis Ford Coppola (dir.), Marlon Brando, Al...</td>\n",
       "      <td>9.2</td>\n",
       "      <td>O Poderoso Chefão</td>\n",
       "      <td>1972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Francis Ford Coppola (dir.), Al Pacino, Robert...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>O Poderoso Chefão II</td>\n",
       "      <td>1974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Christopher Nolan (dir.), Christian Bale, Heat...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Batman: O Cavaleiro das Trevas</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sidney Lumet (dir.), Henry Fonda, Lee J. Cobb</td>\n",
       "      <td>8.9</td>\n",
       "      <td>12 Homens e uma Sentença</td>\n",
       "      <td>1957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                director_and_writers rating  \\\n",
       "0  Frank Darabont (dir.), Tim Robbins, Morgan Fre...    9.2   \n",
       "1  Francis Ford Coppola (dir.), Marlon Brando, Al...    9.2   \n",
       "2  Francis Ford Coppola (dir.), Al Pacino, Robert...    9.0   \n",
       "3  Christopher Nolan (dir.), Christian Bale, Heat...    9.0   \n",
       "4      Sidney Lumet (dir.), Henry Fonda, Lee J. Cobb    8.9   \n",
       "\n",
       "                            title  year  \n",
       "0           Um Sonho de Liberdade  1994  \n",
       "1               O Poderoso Chefão  1972  \n",
       "2            O Poderoso Chefão II  1974  \n",
       "3  Batman: O Cavaleiro das Trevas  2008  \n",
       "4        12 Homens e uma Sentença  1957  "
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "        \"title\": titles, \n",
    "        \"year\": years, \n",
    "        \"rating\": ratings, \n",
    "        \"director_and_writers\":directors_writers\n",
    "    })\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset criado e testado, vamos prosseguir para a etapa de salvar os dados em um arquivo CSV\n",
    "\n",
    "#### Salvando em um arquivo:\n",
    "\n",
    "Com o método to_csv() o pandas vai criar um arquivo .csv do dataset para nós"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('imdb_best_movies.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ética\n",
    "\n",
    "Bom, esse é um tópico interessante, e não se refere necessáriamente a lei, porém as boas maneiras que devemos praticar ao pegar dados de sites que não nos pertençam\n",
    "\n",
    "Pense no contrário: alguem entrando em nossos sites fazendo varreduras, carregando servidores, o que você acharia disso?\n",
    "\n",
    "Estando no outro lado vemos que as vezes isso pode ser incomodo, então preste atenção nestes tópicos:\n",
    "\n",
    "- Caso o site disponibilize uma API, prefira usar ela ao scraping;\n",
    "- Faça requisições de forma moderada, para que afetem de forma menor possível a infraestrurura do proprietário do site\n",
    "- Só pegue os dados que você realmente precisa;\n",
    "- Tente de alguma forma retornar algo que seja de valor para o site, como por exemplo: tráfego;\n",
    "- Tenha em mente seus objetivos, para que fim o dado será usado e que seja diferente do objetivo do site, caso não você estaria apenas duplicando um dado, o que não é interessante para o detentor do conteúdo;\n",
    "\n",
    "É o básico, como viver em sociedade, respeite o próximo, neste caso, o site dele.\n",
    "\n",
    "## Conclusão\n",
    "\n",
    "Bom, vimos do básico a prática do webscraping, claro que no dia a dia veremos situações talvez mais desafiadores que nos levará a procurar num Stackoverflow ou docs das libs, porém a ideia é esta.\n",
    "\n",
    "Precisamos de uma biblioteca para fazer as requisições, que neste caso escolhemos a urllib, e também uma para manipulação do HTML que é a BeautifulSoup, esta última está presente em 100% dos materais que busquei sobre o assunto.\n",
    "\n",
    "E então com as bibliotecas solicitadas, nós precisamos ter conhecimento dos dados que queremos extrair, definir o site alvo e aí olhar sua estrutura HTML para podermos iniciar a raspagem.\n",
    "\n",
    "Tendo as informações já em nossa mão talvez vamos precisar fazer algumas mínimas manipulações de string para termos exatamente o que precisamos.\n",
    "\n",
    "Por fim podemos utilizar os dados, salvar em um arquivo, montar um dataset, aí vai de acordo com o nosso objetivo.\n",
    "\n",
    "Obrigado por lerem, e qualquer dúvida ou sugestão comentem! :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
